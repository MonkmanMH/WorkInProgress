---
title: "Regression models and win expectancy"
output: html_document
---


```{r setup}

library(tidyverse)
library(broom)

library(Lahman)

```

As I write this (2018-07-04) the Seattle Mariners are sitting in second place in the American League West, with a winning percentage of .640 and half a game behind the defending World Series champion Houston Astros. 

The Mariners' relatively small run differential (i.e. how many more runs they have scored than allowed) has been noted all season; 

* at the end of the first month of the season Jake Mailhot (@jakemailhot) at Lookout Landing wrote ["Just how lucky have the Mariners been?"](https://www.lookoutlanding.com/2018/5/1/17308386/mariners-cluster-luck-april-run-differential); 

* on May 25, Sports Illustrated carried an article ["The Mariners Have Been Very Lucky, But They May Also Be Pretty Good"](https://www.si.com/mlb/2018/05/25/seattle-mariners-al-west)

*at the end of May (2018-05-29) John Trupin (@JohnTrupin) on the same site wrote ["The Mariners aren’t the first team built on a run differential-beating bullpen"](https://www.lookoutlanding.com/2018/5/29/17406204/the-mariners-arent-the-first-team-built-on-a-run-differential-beating-bullpen-diaz-colome-nicasio).

* and on July 3, Jeff Sullivan opined ["The Mariners Are Trying to Be the Clutchiest Team on Record"](https://www.fangraphs.com/blogs/the-mariners-are-trying-to-be-the-clutchiest-team-on-record/)



I've written before about run scoring and prevention (here and here); this time, I will compare the individual team performance in a single season.

This seems like a good time to look at the first and simplest of the measures of "win expectation" that have burbled up in the Sabremetric community over the years; the other approaches may be worthy of consideration for a future post.

For the 2018 season, our data source is [Fangraphs' team dashboard](https://www.fangraphs.com/leaders.aspx?pos=all&stats=bat&lg=all&qual=0&type=8&season=2018&month=0&season1=2018&ind=0&team=0,ts&rost=&age=&filter=&players=0). It takes a bit of flinagalling to get what we want.

And for the earlier seasons, we'll rely on the [`Lahman` package](https://cran.r-project.org/web/packages/Lahman/).


## Pythagorean win ratio


Bill James, the godfather of Sabermetrics, developed the Pythagorean win expectation model ([wikipedia page](https://en.wikipedia.org/wiki/Pythagorean_expectation)). The basic idea is that there is a relationship between the runs a team scores ($RS$) and allows ($RA$), and the proportion of the games that they can be expected to win ($WE$). The equation is expressed thus:

$$ WE = RS^2 / (RS^2 + RA^2)$$



FanGraphs ["BaseRuns" page](https://www.fangraphs.com/depthcharts.aspx?position=BaseRuns)

* [Pythagorean Win-Loss](https://www.fangraphs.com/library/principles/expected-wins-and-losses/)

* [Pythagorean Win-Loss](https://www.fangraphs.com/library/team-record-pythagorean-record-and-base-runs/)

* See also ["BaseRuns" definition](https://www.fangraphs.com/library/features/baseruns/) for alternative measure



### function

```{r}

winexp_fun <- function(RS, RA) {
  winratio <- RS^2 / (RS^2 + RA^2)
}

```


test

```{r}

RS <- 4.43
RA <- 4.18


winexp_fun(RS, RA)

```

## Empirical test

```{r}

data(Teams)

head(Teams)

Teams_sel <- Teams %>%
  filter(yearID > 1961) %>%
  rename(RS = R) %>%
  mutate(winpct = W / G)

Teams_sel <- Teams_sel %>%
  mutate(winexp = winexp_fun(RS, RA))


head(Teams_sel)

```


#### plot

Use ggplot2 to look at the relationship between the Pythagorean estimate and the true value.

```{r}

ggplot(data = Teams_sel) +
  geom_density(aes(x = winexp), colour = "red") +
  geom_density(aes(x = winpct), colour = "blue")
                 

ggplot(data = Teams_sel, aes(x = winexp, y = winpct)) +
  geom_point()




```


#### The regression model

```{r}

winexp_mod <- lm(winexp ~ winpct, Teams_sel)

print(winexp_mod)

summary(winexp_mod)

```



Add the model line to the scatter plot

```{r}

ggplot(data = Teams_sel, aes(x = winexp, y = winpct)) +
  geom_point() +
  geom_smooth(method = lm)



```



_interpretation_



Use `broom` to get the model statistics

David Robinson, 2015-03-19, ["broom: a package for tidying statistical models into data frames"](https://www.r-bloggers.com/broom-a-package-for-tidying-statistical-models-into-data-frames/)

* also https://cran.r-project.org/web/packages/broom/vignettes/broom.html 



```{r}

tidy(winexp_mod)

```


```{r}

glance(winexp_mod)

```

```{r}

winexp_mod_tidy <- augment(winexp_mod)
head(winexp_mod_tidy)

```



The Pythagorean model explains a lot of the variation: the R-squared value of `r glance(winexp_mod)$r.squared`




#### Residuals

One of the interesting things we can look at is the difference between the Pythagorean predicted value (`winexp`) and the actual value (`winpct`)--the regression residual. NO IT'S NOT



https://drsimonj.svbtle.com/visualising-residuals


```{r}


ggplot(lm(winpct~winexp, data = Teams_sel)) + 
  geom_point(aes(x = .fitted, y = .resid))


ggplot(winexp_mod_tidy, aes(x = winexp, y = winpct)) +
  geom_point() +
  geom_line(aes(x = .fitted), colour = "red") +
  geom_segment(aes(xend = .fitted, yend = winpct)) +
  geom_point() +
  geom_point(aes(y = .fitted), shape = 1)




ggplot(winexp_mod_tidy, aes(x = winexp, y = winpct)) +
  geom_segment(aes(xend = winpct, yend = .fitted)) +
  geom_point() +
  geom_point(aes(y = .fitted), shape = 1)


```




---


Idea: filter where .se.fit is poor (i.e. big residual), plot -- see where the 2018 Mariners sit on the list

 



---



#### `ggplot2 3.0.0`

This also seemed like a good time to take [the latest version of `ggplot2` (3.0.0)](https://www.tidyverse.org/articles/2018/07/ggplot2-3-0-0/) for a test run.



#### `ggeffects` package

Daniel Lüdecke, 2018-07-04, ["Marginal Effects for Regression Models in R"](https://www.r-bloggers.com/marginal-effects-for-regression-models-in-r-rstats-dataviz/)



