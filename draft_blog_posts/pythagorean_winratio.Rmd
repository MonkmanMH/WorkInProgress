---
title: "Regression models and win expectation"
output: html_document
---


Mid-way through the 2018 Major League Baseball (MLB) season, the Seattle Mariners were sitting in second place in the American League West, with a winning percentage over .600. This success had been in spite of a low run differential (i.e. how many more runs they scored than allowed)--they had been winning a lot of close games.

The Mariners' success had been noted all season; 

* at the end of the first month of the season Jake Mailhot (@jakemailhot) at Lookout Landing wrote ["Just how lucky have the Mariners been?"](https://www.lookoutlanding.com/2018/5/1/17308386/mariners-cluster-luck-april-run-differential); 

* on May 25, Sports Illustrated carried an article ["The Mariners Have Been Very Lucky, But They May Also Be Pretty Good"](https://www.si.com/mlb/2018/05/25/seattle-mariners-al-west)

* at the end of May (2018-05-29) John Trupin (@JohnTrupin) on the same site wrote ["The Mariners aren’t the first team built on a run differential-beating bullpen"](https://www.lookoutlanding.com/2018/5/29/17406204/the-mariners-arent-the-first-team-built-on-a-run-differential-beating-bullpen-diaz-colome-nicasio).

* and on July 3, Jeff Sullivan opined ["The Mariners Are Trying to Be the Clutchiest Team on Record"](https://www.fangraphs.com/blogs/the-mariners-are-trying-to-be-the-clutchiest-team-on-record/)

Of course, [regression toward the mean](https://en.wikipedia.org/wiki/Regression_toward_the_mean) is a thing, and later in the season their win/loss ratio more closely reflected their run differential. 


---

I've written before about run scoring and prevention (index [here](http://bayesball.blogspot.com/search/label/run%20scoring)). This time, I will look at the simplest of the measures of "win expectation" that have burbled up in the sabermetric community over the years; the other approaches may be worthy of consideration for a future post. This exercise will give us a way to look at the ways that the statistical programming language R works with regression models.


## Pythagorean win ratio


Bill James, the godfather of sabermetrics, developed the Pythagorean win expectation model ([wikipedia page](https://en.wikipedia.org/wiki/Pythagorean_expectation)). The basic idea is that there is a relationship between the runs a team scores ($RS$) and allows ($RA$), and the proportion of the games that they can be expected to win ($WE$). The equation is expressed thus:

$$ WE = RS^2 / (RS^2 + RA^2)$$



### function

Let's write a little function for this equation...in that way, we can save some typing later.

```{r}

winexp_fun <- function(RS, RA) {
  RS^2 / (RS^2 + RA^2)
}

```


## The data


First, we'll load the packages we need. Note that `tidyverse` contains multiple packages, including the graphing package `ggplot2` and the data wrangling package `dplyr`. We'll also load `broom`, which facilitates manipulation of the model outputs.

For this analysis, we'll use the Major League Baseball data from 1961 through 2016--a total of 1476 team seasons. To get the data, we'll rely on the [CRAN](https://cran.r-project.org/) version of the [`Lahman` package](https://cran.r-project.org/web/packages/Lahman/), which will (at this writing) take us through the 2016 season. 




```{r setup}

library(tidyverse)
library(broom)

library(Lahman)

```


The code chunk below accesses the `Teams` table from the Lahman database, and wrangles it a bit, and adds (though the `dplyr::mutate` function) two new variables: the team's winning percentage, and using the `winexp_fun` function we wrote above, the win expectation.

```{r}

data(Teams)

Teams_sel <- Teams %>%
  filter(yearID >= 1961) %>%
  rename(RS = R) %>%
  mutate(winpct = W / G, 
         winexp = winexp_fun(RS, RA))


```


#### plot

Now we'll use `ggplot2` to look at the relationship between the Pythagorean estimate of win expectation and the actual value of winning percentage. We can do this in a couple of ways: one is to overlay the density plots of the two variables, and the other is an X-Y scatterplot.

First the density plot.

```{r}

ggplot(data = Teams_sel) +
  geom_density(aes(x = winexp, colour = "winexp"), show.legend = FALSE) +
  stat_density(aes(x = winpct, colour = "winpct"),
               geom = "line", position = "identity", size = 0) +
  guides(colour = guide_legend(override.aes=list(size=1)))

# tips from https://stackoverflow.com/questions/29563375/plotting-line-legend-for-two-density-curves-with-ggplot2


```

In the above plot, we can see that there's not a perfect match--first, there's gaps between the two lines at either tail of the curve. But the blue line (representing the actual winning percentage) isn't a smooth curve at the top--there's a hollow around .500, and increased proportions on either side. Something to look at another day!


Next, the scatter plot. Because we are going to return to the foundations of this plot (i.e. the calculated win expectancy as the X axis and the end-of-season final winning percentage plotted on the Y axis), we'll create a blank frame in an object called `we_scatterplot`. With this object already created, we can build a variety of plots by simply calling the base object. (It's not lazy, it's _efficient_.)

Note that there's a few things going on here:

* the use of the `geom_blank` function; usually, we would call `geom_point` for a scatter plot, but in this case we don't want to see the points. 

* the `coord_fixed` means that the X and Y scales have the units represented by equal length on both (one tenth of a point is the same length on each). 

* the `scale_x_continuous` function and its parallel for y set the grid marks and length of the two axes.

There are two ways to approach this. In the first, the object contains no data whatsoever; if we want a scatter plot, we can add the data in the `geom_point` call.

```{r}

we_scatterplot <- ggplot() +
  geom_blank() +
  coord_fixed() +
  scale_x_continuous(breaks = seq(0.2, 0.8, by = 0.1),
                     limits = c(0.2, 0.75)) +
  scale_y_continuous(breaks = seq(0.2, 0.8, by = 0.1),
                     limits = c(0.2, 0.75)) 

we_scatterplot


we_scatterplot +
  geom_point(data = Teams_sel, mapping = aes(x = winexp, y = winpct))


```


The second approach will work more effectively for our project, since we are returning to the same underlying framwork, and plotting different representations of the same data. So the data gets embedded in the object during its creation, but since we're used `geom_blank` we don't see any of the data.


```{r}

we_scatterplot <- ggplot(data = Teams_sel, aes(x = winexp, y = winpct)) +
  geom_blank() +
  coord_fixed() +
  scale_x_continuous(breaks = seq(0.2, 0.8, by = 0.1),
                     limits = c(0.2, 0.75)) +
  scale_y_continuous(breaks = seq(0.2, 0.8, by = 0.1),
                     limits = c(0.2, 0.75)) 

we_scatterplot

```




The `we_scatterplot` object contains the `winexp` and `winpct` data points, so we can summon them in the `geom_` that we want. 

Now, we'll render that object but using the `geom_point` so we can see the winexp and winpct values.

```{r}


we_scatterplot +
  geom_point()



```

In the above plot it's easy to see the strong relationship between the win expectation and the winning percentage. It's not a perfect relation; over a 162 game season, there is still room for variation.


#### The regression model

Let's model that relationship. R makes regression model building easy; the `lm` function is all that it takes. The `print()` and `summary()` functions let us see the key elements from the model.


```{r}

winexp_mod <- lm(winexp ~ winpct, Teams_sel)

print(winexp_mod)

summary(winexp_mod)

```



R also makes it very easy to add the regression model line to the scatter plot, with the `geom_smooth()` function. Note that we have to specify the `method = lm`; the default is a [LOESS smoothing](https://en.wikipedia.org/wiki/Local_regression) curve.


```{r}

we_scatterplot +
  geom_point() +
  geom_smooth(method = lm)


```



_interpretation_



Use `broom` to get the model statistics

David Robinson, 2015-03-19, ["broom: a package for tidying statistical models into data frames"](https://www.r-bloggers.com/broom-a-package-for-tidying-statistical-models-into-data-frames/)


* also https://cran.r-project.org/web/packages/broom/vignettes/broom.html 



```{r}

tidy(winexp_mod)

```


```{r}

glance(winexp_mod)

```

```{r}

winexp_mod_tidy <- augment(winexp_mod)
head(winexp_mod_tidy)

```



The Pythagorean model explains a lot of the variation: the R-squared value of `r glance(winexp_mod)$r.squared`



One of the interesting things we can look at is the difference between the Pythagorean predicted value (`winexp`) and the actual value (`winpct`).



In this plot, the cases above the red line are where teams have outperformed their win expectancy, and those below the line have failed to win as many games as the Pythagorean model would predict.

What we are really interested in are the extreme cases--some of those single dots that are visible outside the main cloud that is clustered around the line.


```{r}


we_scatterplot +
  geom_point() +
  geom_segment(aes(x = 0.25, xend = 0.75, y = 0.25, yend = 0.75), colour = "red", size = 1)


```



And just for fun, let's compare the "unity" line shown in red (where `winexp` == `winpct`) and the line of best fit created by the regression model (in blue). There's a slight discrepancy (figure out what it means--is it that teams at the bottom left are over-performing Pythagorean, suggesting winning more close games, while those at the top are underperforming, meaning they are winning more blowouts?)


```{r}

we_scatterplot +
  geom_point() +
  geom_segment(aes(x = 0.25, xend = 0.75, y = 0.25, yend = 0.75), colour = "red", size = 1) +
  geom_smooth(method = lm)

```



### Further reading

FanGraphs ["BaseRuns" page](https://www.fangraphs.com/depthcharts.aspx?position=BaseRuns)

* [Pythagorean Win-Loss](https://www.fangraphs.com/library/principles/expected-wins-and-losses/)

* [Pythagorean Win-Loss](https://www.fangraphs.com/library/team-record-pythagorean-record-and-base-runs/)

* See also ["BaseRuns" definition](https://www.fangraphs.com/library/features/baseruns/) for alternative measure

Jay Heumann, ["An improvement to the baseball statistic 'Pythagorean Wins'"](https://content.iospress.com/download/journal-of-sports-analytics/jsa0018?id=journal-of-sports-analytics%2Fjsa0018), _Journal of Sports Analytics_ 2 (2016) 49–59


---


## ETC - future analysis possibilities

Idea: filter where .se.fit is poor (i.e. big residual), plot -- see where the 2018 Mariners sit on the list


Idea: regression to the mean over the course of a season, via a Bayesian MCMC approach 



---



#### `ggplot2 3.0.0`

This also seemed like a good time to take [the latest version of `ggplot2` (3.0.0)](https://www.tidyverse.org/articles/2018/07/ggplot2-3-0-0/) for a test run.



#### `ggeffects` package

Daniel Lüdecke, 2018-07-04, ["Marginal Effects for Regression Models in R"](https://www.r-bloggers.com/marginal-effects-for-regression-models-in-r-rstats-dataviz/)



